<!--Autogenerated using pandoc <md files> -o <outfile.html> --toc --template template.html5 --css style.css  --section-divs -->

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Getting started with Unity and VR</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
  <div>
    <div id="ah-sidebar">
      <ul id="toc">
                <ul><li><a href="#getting-started-with-adhawk-eye-tracking-and-the-meta-quest-2-vr-headset">Getting started with AdHawk eye tracking and the Meta Quest 2 VR headset</a></li><li><a href="#what-is-eye-tracking">What is eye tracking?</a><ul><li><a href="#human-eye-movements">Human eye movements</a></li><li><a href="#how-does-eye-tracking-work">How does eye tracking work?</a></li><li><a href="#eye-tracking-applications">Eye tracking applications</a></li></ul></li><li><a href="#adhawk-eye-tracking">AdHawk eye tracking</a><ul><li><a href="#system-requirements">System requirements</a></li><li><a href="#hardware-components">Hardware components</a></li><li><a href="#software-components">Software components</a><ul><li><a href="#installing-the-sdk">Installing the SDK</a></li><li><a href="#working-with-unity">Working with Unity</a></li></ul></li><li><a href="#fitting-the-meta-quest-2-headset">Fitting the Meta Quest 2 headset</a></li><li><a href="#calibration">Calibration</a></li></ul></li><li><a href="#creating-a-vr-app-with-eye-tracking">Creating a VR app with eye tracking</a><ul><li><a href="#ideal-vr-app-flow">Ideal VR app flow</a></li><li><a href="#designing-your-vr-scenes">Designing your VR scenes</a><ul><li><a href="#field-of-view">Field of view</a></li><li><a href="#positioning-and-sizing-interactive-elements">Positioning and sizing interactive elements</a></li></ul></li><li><a href="#important-unity-components-and-scripts">Important Unity components and scripts</a><ul><li><a href="#gazeeventsystem">GazeEventSystem</a></li></ul></li><li><a href="#gazedetector">GazeDetector</a><ul><li><a href="#eyetrackerapi">EyeTrackerAPI</a></li><li><a href="#controllerinputhandler-and-keyboardinputhandler">ControllerInputHandler and KeyboardInputHandler</a></li></ul></li><li><a href="#learn-from-the-example-scenes">Learn from the example scenes</a><ul><li><a href="#basic-eye-tracking-scene-3-your-app-here">Basic eye tracking scene (“3 Your App Here”)</a></li><li><a href="#interacting-with-objects-balloonpopping">Interacting with objects (“BalloonPopping”)</a></li><li><a href="#saccade-selection-saccadeselectexample">Saccade selection (“SaccadeSelectExample”)</a></li><li><a href="#button-pressing-buttonpress2dworldspace">Button pressing (“ButtonPress2DWorldspace”)</a></li></ul></li><li><a href="#testing-gaze-with-a-mouse-during-development">Testing gaze with a mouse during development</a></li></ul></li><li><a href="#key-steps">Key steps</a></li><li><a href="#working-with-data">Working with data</a><ul><li><a href="#gaze-data">Gaze data</a></li><li><a href="#event-data">Event data</a></li></ul></li><li><a href="#troubleshooting">Troubleshooting</a><ul><li><a href="#gaze-vector-is-not-accurate-after-running-a-calibration">Gaze vector is not accurate after running a calibration</a></li><li><a href="#gaze-vector-is-not-accurate-after-headset-slips">Gaze vector is not accurate after headset slips</a></li><li><a href="#oculus-link-button-does-not-show-up">Oculus Link button does not show up</a></li><li><a href="#headset-stays-gray-or-black-when-worn">Headset stays gray or black when worn</a></li><li><a href="#problems-with-scene-orientation">Problems with scene orientation</a></li><li><a href="#objects-dont-react-to-gaze-events">Objects don’t react to gaze events</a></li></ul></li><li><a href="#glossary">Glossary</a></li></ul>
              </ul>
    </div>
    <div id="ah-main">
<!--https://stackoverflow.com/questions/42773587/code-block-formatting-issues-with-bootstrap-and-pandoc-->
<section id="getting-started-with-adhawk-eye-tracking-and-the-meta-quest-2-vr-headset" class="level1"><h1>Getting started with AdHawk eye tracking and the Meta Quest 2 VR headset</h1></section><section id="what-is-eye-tracking" class="level1"><h1>What is eye tracking?</h1><p>Eye tracking is the process of observing eye behavior so you can learn about the point of gaze. Eye tracking can be used to:</p><ul><li>learn what someone is looking at</li><li>learn about aspects of someone’s mental state (e.g., fatigue, concussion, level of focus, etc.)</li></ul><section id="human-eye-movements" class="level2"><h2>Human eye movements</h2><p>Before diving into the AdHawk SDK and writing an application that incorporates eye tracking data, it helps to know about some of the more common eye movements.</p><section id="fixations" class="level4"><h4>Fixations</h4><p>One of the most common eye events occurs when the eyes aren’t moving. <em>Fixations</em> occur when we focus on something for a period of time. Fixations range in duration from tens of milliseconds to several seconds.</p></section><section id="saccades" class="level4"><h4>Saccades</h4><p><em>Saccades</em> occur when our eyes move between fixations. These movements are the fastest movements the human body can perform; they can reach a peak velocity of 500 degrees/second (although the amplitude of saccades are usually in the 4 to 20 degree range).</p></section><section id="smooth-pursuit" class="level4"><h4>Smooth pursuit</h4><p><em>Smooth pursuit</em> is another form of eye movement but one which requires something to follow (unlike saccades which can happen without stimuli). Consider a bird flying across the sky: our brain enters a feedback loop where we can smoothly track the bird as it moves. Smooth pursuit velocities range from 10 to 30 degrees/second (compared with the much faster saccades).</p></section><section id="blinks" class="level4"><h4>Blinks</h4><p>Blinking closes the eyelids to moisten the eye. We often think of blinking as not involving eyeball movement, but while fixating on something straight ahead, the eyeball actually rotates downward slightly during a blink.</p><blockquote><p>Blinks are an event within the AdHawk API; you can register to receive notifications when they occur.</p></blockquote></section><section id="vestibulo-ocular-reflex-vor" class="level4"><h4>Vestibulo-ocular reflex (VOR)</h4><p>The vestibulo-ocular reflex acts to stablize your gaze (on a visual target) while your head is moving. This helps stabilize the image on your retina as the environment may be changing. To see VOR in action, fixate on a word in this sentence and move your head from right to left; notice how your gaze remains locked to the word while your head moves?</p><p>One of the calibration methods supported by AdHawk relies on VOR (‘fixed gaze’).</p></section><section id="vergence" class="level4"><h4>Vergence</h4><p><em>Vergence</em> involves eyes moving in opposite directions. This is done to converge or diverge, usually to focus on items that are close up or far away, respectively.</p><blockquote><p>Vergence information is available as part of the <a href="#gaze-stream">gaze data stream</a>.</p></blockquote></section></section><section id="how-does-eye-tracking-work" class="level2"><h2>How does eye tracking work?</h2><p>You can generally tell where someone is looking by looking at their eyes. Eye tracking is nothing more than a really accurate, really fast way of the same thing.</p><p>Some eye tracking systems use a combination of small cameras pointed at your eyes and image analysis to figure out where you’re looking.</p><p>AdHawk eye tracking doesn’t use cameras and image processing (both require a fair bit of power), instead using incredibly small (MEMS-scale) mirrors to shine a lower power IR light across your eyes. Reflections from your eyes are captured by photodetectors and feed into a mathematical model. This model allows us to determine where you’re looking very accurately (&lt; 1 degree MAE) and at high rates (up to 500 Hz).</p><figure><img src="./assets/htn/mems.png" width="400" alt="" /><figcaption>MEMS mirror</figcaption></figure></section><section id="eye-tracking-applications" class="level2"><h2>Eye tracking applications</h2><p>Eye tracking can be used in several different ways:</p><ul><li><strong>Interaction</strong>. In the same way as a mouse or your finger can move across a screen, so to can your gaze act as a pointer. Consider actions like blinks to be like mouse clicks or taps. This sort of direct interaction can be as rich as other modalities, and can allow us to communicate our interest and intent through eye behavior.</li><li><strong>Observation</strong>. Eye behavior is becoming increasingly valuable in unlocking mental and cognitive states. Eye tracking can be used to observe and collect information about these states in an unobtrusive way. Data collected can be used to help identify, diagnose, or assess recovery for certain medical conditions (e.g., concussion, dementia, etc.).</li><li><strong>Research</strong>. Although a specialized form of observation, eye tracking has been used in market research (to learn more about people’s behavior in retail environments, for example) and lab settings (e.g., psychology, neuroscience, usability, etc.) for a long time.</li></ul><p>We’ve posted a few interesting examples of eye tracking on our <a href="https://sites.google.com/adhawkmicrosystems.com/hack-the-north-help-center/home">Hack the North website</a>.</p></section></section><section id="adhawk-eye-tracking" class="level1"><h1>AdHawk eye tracking</h1><p>Traditional eye tracking systems point a camera at your eye and use the image of your pupil to determine where you’re looking.</p><p>The AdHawk eye tracking system does not use a camera to calculate your gaze. Instead, a path of low power IR light is drawn on your eyeball and photodetectors capture the reflections. Those glints tell enough about the position and orientation of your eyes to figure out where you’re looking.</p><p>This system has been inserted into the Meta Quest 2 headset to provide high quality eye tracking within a VR environment. With this approach we can calculate your gaze (where you’re looking), the vergence angle (angle between your individual eye gaze vectors), and pupil size and position.</p><p>Eye tracking data is made available to your application through several Unity components and scripts.</p><section id="system-requirements" class="level2"><h2>System requirements</h2><p>To create an eye tracking application that runs in VR on the Meta Quest 2 headset, you’ll need a reasonably powerful, Windows-based computer capable of driving the headset display over a link cable (provided as part of the kit).</p><p>On the computer, you’ll use Unity to develop your application, then run it on the Meta Quest 2 headset using the link cable. The computer must:</p><ul><li>run Windows 10 or Windows 11</li><li>have 2 USB-A ports (one for the link cable, one for the eye tracker)<ul><li>the Oculus Link port on the computer side needs to be a USB 3.0 port</li></ul></li><li>have a discrete GPU (see <a href="https://store.facebook.com/help/quest/articles/headsets-and-accessories/oculus-link/oculus-link-compatibility/">Meta’s list of supported GPUs</a>)</li><li>have <a href="https://unity3d.com/get-unity/download/archive">Unity 2021.1.21f1</a> installed (that’s the version we use; others may work) with:<ul><li>Windows Build Support</li></ul></li><li>Visual Studio Community 2019 (or other Unity-compatible debugging tool/text editor)<ul><li>this can be installed through Unity Hub</li></ul></li></ul><p>Many newer laptops do not have two dedicated USB ports or a discrete GPU suitable for this work, so a recent gaming laptop may be required.</p><p>You must also have a smartphone with the Meta Quest (Oculus) mobile app (for <a href="https://apps.apple.com/us/app/oculus-vr/id1366478176">iOS 10+</a>; <a href="https://play.google.com/store/apps/details?id=com.oculus.twilight">Android 5.0+</a>) installed. A Meta account is also required but can be created within the mobile app. The Meta Quest 2 headset will connect to your smartphone using Bluetooth, and use the credentials you’ve created or signed in with on the mobile app.</p><p>For more information about system requirements of the Meta Quest 2, see the <a href="https://store.facebook.com/help/quest/articles/headsets-and-accessories/oculus-link/oculus-link-compatibility/">Oculus Link PC system requirements</a>.</p></section><section id="hardware-components" class="level2"><h2>Hardware components</h2><p>The hardware components within the AdHawk eye tracking system are:</p><ol type="1"><li><strong>Eye tracking inserts</strong>. The eye tracking components (scanner, photodetectors) are contained within modules that are placed over and around the Meta Quest 2 headset lens mounts.</li></ol><ul><li>a small PCB sits within the headset and manages the eye tracking system and data streams</li><li>a USB cable connecting the eye tracking system PCB and your computer</li></ul><ol type="1"><li><strong>Meta Quest 2 headset</strong>. The eye tracking system works in tandem with the headset.</li></ol><ul><li>a USB cable connecting the headset and your computer</li></ul></section><section id="software-components" class="level2"><h2>Software components</h2><p>To create a VR application with eye tracking, you will use:</p><ul><li>Unity (and a few dependencies: <em>Visual Studio Community</em> and <em>Windows Build Support</em>; see above)</li><li>the AdHawk Unity SDK (available on AdHawk’s <a href="https://sites.google.com/adhawkmicrosystems.com/hack-the-north-help-center/software">Hack the North site</a>)</li></ul><section id="installing-the-sdk" class="level3"><h3>Installing the SDK</h3><p>The AdHawk Unity SDK is packaged as a .zip file:</p><ul><li>unzip the package to a directory on your computer</li><li>open the project from within Unity Hub</li></ul><p>We have provided Unity projects which incorporate key parts of the AdHawk Unity SDK. These projects demonstrate how to:</p><ul><li>set up and calibrate eye tracking within a VR app</li><li>set up your VR scenes within Unity</li><li>receive, process, and act on eye tracking data</li><li>interact with scene elements using gaze</li></ul></section><section id="working-with-unity" class="level3"><h3>Working with Unity</h3><p>Ensure that you have a Unity Personal license created and installed. A personal license can be added through Unity Hub (Profile &gt; Manage licenses &gt; Add).</p></section></section><section id="fitting-the-meta-quest-2-headset" class="level2"><h2>Fitting the Meta Quest 2 headset</h2><p>Eye tracking systems are largely comprised of physical sensors positioned near the eyes themselves. To ensure great eye tracking, you need to optimize the position of those sensors.</p><p>For a good fit using the Meta Quest 2 headset:</p><ul><li><strong>Remove your glasses</strong>. While very narrow glasses do fit within the Meta Quest 2 headset, prescription glasses prevent eye tracking. For more common prescriptions, some lenses are available from the AdHawk sponsor booth.</li><li><strong>Adjust the IPD setting for the headset</strong>. The interpupillary distance (IPD) is the distance between your pupils when you’re looking into the distance. The Meta Quest 2 headset supports three IPD values: 1 (58 mm), 2 (63 mm) and 3 (68 mm). Meta describes <a href="https://store.facebook.com/help/quest/articles/getting-started/getting-started-with-quest-2/ipd-quest-2/">how to adjust this on their support site</a>.</li><li><strong>Tighten the straps</strong>. To reduce slipping due to the weight of the headset, you should adjust the straps so that the headset is snug but not tight. <a href="https://store.facebook.com/help/quest/articles/getting-started/getting-started-with-quest-2">General instructions for fitting the headset</a> are provided by Meta on their support site.</li><li><strong>Consider using the provided spacer</strong>. The Meta Quest 2 kit includes a ‘glasses spacer’ which can be stacked between the headset and stock facial interface. Most people don’t require this, but if other adjustments don’t help, consider trying it.</li></ul></section><section id="calibration" class="level2"><h2>Calibration</h2><p>The process of calibration allows the system to relate a measured gaze to the virtual environment. That is, if you’re looking up and to the left, we want to make sure that the gaze information we provide your application is correct.</p><p>In general, <strong>you should run a calibration whenever someone puts on the headset</strong>. You should also support someone explicitly running a calibration from within your application if it makes sense (e.g., where progress or work would be lost if they had to restart in order to calibrate). You can do this by revisiting the calibration scene within your Unity app.</p><p>The example projects demonstrate best practices for your VR application by having a calibration scene after the app intro scene. This allows users to get set up for the best eye tracking possible. During development you may want to perform a re-center; this is a quick way to recenter your gaze within a scene. Re-centering isn’t as comprehensive as a full calibration, but can do in a pinch during development. TODO: Add reference to re-centering keyboard shortcut.</p></section></section><section id="creating-a-vr-app-with-eye-tracking" class="level1"><h1>Creating a VR app with eye tracking</h1><p>The Meta Quest 2 headset you’ve been provided includes an AdHawk eye tracking system. With this hardware and by using the specialized eye tracking components and scripts we’ve provided as part of the Unity sample project, you can:</p><ul><li>calibrate the eye tracker</li><li>use gaze information in real-time within your Unity application</li><li>receive notifications about events such as blinks to help with interaction</li></ul><section id="ideal-vr-app-flow" class="level2"><h2>Ideal VR app flow</h2><p>When creating an eye tracking app in VR, there are a few steps that need to occur in a specific order. Each of these steps are represented as scenes in your Unity project.</p><ol type="1"><li><strong>Welcome the user</strong>. It makes sense to introduce the user to the environment, especially when it requires some specialized setup like calibrating the eye tracker. This scene orients the user to your application and how to use it.</li><li><strong>Perform a calibration</strong>. As a rule of thumb, each time someone puts on the headset, you should run a calibration. Moving the headset too much—such as taking it off and putting it on—will change the position of the eye tracker relative to the eyes. When this happens, a calibration will fix things up.</li><li><strong>Your application</strong>. This is where you shine. But keep in mind that there should likely be a way to explicitly trigger a calibration within the application in case eye tracking isn’t great. By allowing the user to calibration from any point in your application, you allow them to retain their progress.</li></ol></section><section id="designing-your-vr-scenes" class="level2"><h2>Designing your VR scenes</h2><section id="field-of-view" class="level3"><h3>Field of view</h3><p>Eye tracking with the Meta Quest 2 is best within a 40 degree (horizontal) by 25 degree (vertical) area. While eye tracking is possible outside of this zone, there may be less accuracy. “Eye-interactive content”—content that can be interacted with using eye movements—within a VR environment should be placed within this ideal eye tracking zone.</p><blockquote><p>Interactions involving eye tracking should be located centrally to keep things comfortable.</p></blockquote><figure><img src="./assets/htn/vr_fields_of_view.png" width="800" alt="" /><figcaption>Fields of view in VR</figcaption></figure></section><section id="positioning-and-sizing-interactive-elements" class="level3"><h3>Positioning and sizing interactive elements</h3><p>There are a few different scenarios to consider when positioning and sizing eye-interactive elements.</p><p>Interactive elements that are in a fixed position relative to the user (e.g., a heads up display (HUD)) should be placed 1 m from the user viewpoint. That distance has been shown to be comfortable with respect to vergence (how your eyes move to accommodate for depth).</p><p>If your VR application supports moving around the scene and interactive elements are part of that scene, think about the size of those interactive elements. If users will likely be far away, consider making the elements (or their ‘hit boxes’) larger and attention-grabbing.</p></section></section><section id="important-unity-components-and-scripts" class="level2"><h2>Important Unity components and scripts</h2><p>We’ve provided components to help you get eye tracking working quickly. Lean on these components</p><section id="gazeeventsystem" class="level3"><h3>GazeEventSystem</h3><p>The <code>GazeEventSystem</code> component allows gaze events to be received as mouse events. This means that:</p><ul><li>eye movements will be translated into mouse-like movements for 2D canvas objects in world space</li><li>blinks will be translated into mouse clicks</li><li>allows interaction with 3D colliders that have the GazeDetector component</li></ul></section></section><section id="gazedetector" class="level2"><h2>GazeDetector</h2><p>The <code>GazeDetector</code> component is an event system compatible interaction handler. It allows you make objects that are aware of gaze and gaze events (such as <code>OnGazeEnter</code>, <code>OnGazeLeave</code>, and blink events).</p><section id="eyetrackerapi" class="level3"><h3>EyeTrackerAPI</h3><p>The <code>EyeTrackerAPI</code> component is a singleton that allows for access to gaze data, events streams, and other eye tracking-related functions. This will help you use AdHawk eye tracking within your application, and acts as a bridge between the lower level eye tracking API and Unity.</p></section><section id="controllerinputhandler-and-keyboardinputhandler" class="level3"><h3>ControllerInputHandler and KeyboardInputHandler</h3><p>The <code>ControllerInputHandler</code> and <code>KeyboardInputHandler</code> components simplify working with the Meta Quest 2 controllers. In some situations you may need to use the physical controllers, but keep in mind that gaze and associated eye events (such as blinks) are a great way to interact as well.</p></section></section><section id="learn-from-the-example-scenes" class="level2"><h2>Learn from the example scenes</h2><p>Each of the example scenes provided follow the recommended flow of: welcoming the user, performing a calibration, and then presenting the main application scene. They also show how to use the components we’ve provided to help get robust eye tracking into your app.</p><section id="basic-eye-tracking-scene-3-your-app-here" class="level3"><h3>Basic eye tracking scene (“3 Your App Here”)</h3><p>This scene contains all of the base components that you’ll need to get started with eye tracking. When building your application, start from here and build out.</p></section><section id="interacting-with-objects-balloonpopping" class="level3"><h3>Interacting with objects (“BalloonPopping”)</h3><p>This scene contains a few ‘balloons’ and demonstrates the gaze detector in 3D space. If you look at the balloon it will expand. If you look away it will stop growing. If you blink, the balloon you’re looking at will pop.</p><p>This code has a <code>GazeEventSystem</code> and uses <code>GazeDetector</code>.</p></section><section id="saccade-selection-saccadeselectexample" class="level3"><h3>Saccade selection (“SaccadeSelectExample”)</h3><p>This scene demonstrates the <code>GazeEventSystem</code> and <code>GazeDetector</code> components. In this scene, you can look at one object, drag it down—with your gaze—to another object as a way to ‘apply’ it. This demonstrates multi-point interaction.</p><p>This code has a <code>GazeEventSystem</code> and uses <code>GazeDetector</code>.</p></section><section id="button-pressing-buttonpress2dworldspace" class="level3"><h3>Button pressing (“ButtonPress2DWorldspace”)</h3><p>This example shows off <code>GazeEventSystem</code> and <code>GazeDetector</code> in a 2D canvas. In this scene, if you look at one button, another button appears. Looking at the top button swaps the appearance of that button while you look at it. If you blink while looking at the top button, the bottom buttons reappear.</p><p><strong>An important note:</strong> There is a lot of power in the Unity event system when used with the Unity Inspector and its ability to drag and drop components into event system slots. This <em>drag and drop ‘coding’</em> gets used a lot and is very powerful for prototyping within Unity.</p><p>This example will show you how to have an object react to a blink.</p></section></section><section id="testing-gaze-with-a-mouse-during-development" class="level2"><h2>Testing gaze with a mouse during development</h2><p>The AdHawk Unity SDK includes several components to route eye tracking data to your application. However, sometimes it is useful to be able to run your application on your computer (not on the VR headset); in these situations, you can configure a mouse to simulate gaze.</p><ul><li>within the Unity Inspector, configure the eye tracking source (<code>ET Source</code>) for <code>EyeTrackerAPI</code></li><li>you can choose to use a mouse or the backend (the headset’s eye tracking source)</li><li>when you build the VR version of the application, <code>ET Source</code> will be explicitly set to Backend (which uses the AdHawk eye tracking hardware within the headset)</li></ul></section></section><section id="key-steps" class="level1"><h1>Key steps</h1><p>There are a few steps required to get eye tracking working well within your application:</p><ol type="1"><li>Ensure that AdHawk Backend is running on your computer. It will be available in the System Tray.</li><li>The headset must be plugged in:</li></ol><ul><li>both the Oculus VR headset and the eye tracking insert inside the viewport must be connected via the provided USB cables to your computer</li></ul><ol type="1"><li>The Oculus app should be running on the development computer.</li><li>Enable Oculus Link (Meta has <a href="https://www.youtube.com/watch?v=IQrPiTlsU9I">a video to explain how to do this</a>).</li></ol></section><section id="working-with-data" class="level1"><h1>Working with data</h1><p>Eye tracking data is provided through <code>EyeTrackerAPI</code>. This is a singleton object that you can use within your Unity applications to access:</p><ul><li>EyeTrackerAPI.Instance.Streams.Gaze</li><li>EyeTrackerAPI.Instance.Streams.Events</li></ul><section id="gaze-data" class="level2"><h2>Gaze data</h2><p>Gaze data is accessed via <code>EyeTrackerAPI.Instance.Streams.Gaze.Position</code>.</p></section><section id="event-data" class="level2"><h2>Event data</h2><p>Blink events are likely the most useful when developing interactive gaze-enabled VR applications. Such events are represented by <code>EyeTrackerAPI.Instance.Events.Blink</code>.</p><p><code>EyeTrackerAPI.Instance.DidBlinkLastFrame</code> can be quite useful when working with blink events. It is a boolean and is true if the user finished a blink in the last frame.</p><p><code>GazeEventSystem</code> has a tool which will provide a pointerClickInteraction at the position that the gaze was at just before the blink started. This can be useful in processing a blink event (e.g., to determine what the user is trying to interact with).</p><p>When handling blink events, it is important to note that blinks aren’t instantaneous: there is a start time, a middle, and an end time. Between the start and end time, the gaze vector for that eye will move around a bit in unpredictable ways (this is because the eye moves when eyelids close). Users should take this into account when using <code>DidBlinkLastFrame</code>—you may want to ignore the gaze vector during the blink itself and perhaps for a short period of time thereafter.</p></section></section><section id="troubleshooting" class="level1"><h1>Troubleshooting</h1><section id="gaze-vector-is-not-accurate-after-running-a-calibration" class="level2"><h2>Gaze vector is not accurate after running a calibration</h2><p>If the gaze vector doesn’t appear to be tracking your actual gaze accurately, even after running a calibration:</p><ul><li>check your headset fit (see the <a href="#fitting-the-meta-quest-2-headset">fitting section of this guide</a>)</li><li>re-run calibration</li></ul></section><section id="gaze-vector-is-not-accurate-after-headset-slips" class="level2"><h2>Gaze vector is not accurate after headset slips</h2><p>When the position of the headset changes significantly relative to your eyes, eye tracking can suffer. Check your headset fit (see the <a href="#fitting-the-meta-quest-2-headset">fitting section of this guide</a>) as a starting point, in particular:</p><ul><li>tighten the headset so that it doesn’t shift or droop</li><li>try the spacer on the headset (or remove it if you’ve already been using it)</li></ul></section><section id="oculus-link-button-does-not-show-up" class="level2"><h2>Oculus Link button does not show up</h2><p>If the Oculus Link button is no longer available within the Oculus Quick Settings:</p><ul><li>unplug the headset side of the Oculus Link cable</li><li>firmly replug in the Oculus Link cable on the headset side</li></ul></section><section id="headset-stays-gray-or-black-when-worn" class="level2"><h2>Headset stays gray or black when worn</h2><p>When the headset stays gray or black after you’ve put it on, it isn’t recognizing that you’re using it. You can:</p><ul><li>ensure the proximity sensor (within the headset) is not blocked</li><li>restart the headset (hold power button until it turns off; turn it back on again)</li><li>ensure the battery is charged (battery life is about 2–3 hours of use; this will be longer if use isn’t constant or the headset is plugged into a PC)</li></ul></section><section id="problems-with-scene-orientation" class="level2"><h2>Problems with scene orientation</h2><p>When you start your application, are you positioned away from where you expected? Are you looking in the wrong direction? If you encounter these issues:</p><ul><li>look in the right direction and position yourself properly</li><li>start Oculus Link</li><li>start your app after Oculus Link is running</li></ul></section><section id="objects-dont-react-to-gaze-events" class="level2"><h2>Objects don’t react to gaze events</h2><p>If you’re finding that an object in your scene isn’t responsive to gaze events:</p><ul><li>ensure there isn’t another collider in front of your object</li><li>check that the object uses GazeDetector</li></ul></section></section><section id="glossary" class="level1"><h1>Glossary</h1><p><strong>AdHawk MindLink</strong>. AdHawk MindLink glasses include a full eye tracking system within the frames.</p><p><strong>Anti-saccade</strong>. A saccade <em>away</em> from a stimulus.</p><p><strong>ArUco markers</strong>. Specialized (graphical) markers designed to facilitate computer vision. They are blocky black and white grids with a black border.</p><p><strong>Auto-tune</strong>. A process to optimize where the eye tracking system will scan on someone’s eyes.</p><p><strong>Calibration</strong>. The process by which the relationship between a user’s eye tracking setup—and thus their gaze—and the world is calculated, allowing for the ‘real world’ gaze target to be determined.</p><p><strong>Device calibration</strong>. Infrequently, he eye tracker needs to be calibrated against a known environment or set of inputs. This is different than regular calibration (see above) and involves a calibration <em>fixture</em>. If you need to perform a device calibration (e.g., you get a return code of 17 when calling into the API), visit AdHawk’s sponsor booth.</p><p><strong>Eye tracking</strong>. The process of inferring the gaze point (and, in general, eye behavior) by using technology to observe someone’s eyes.</p><p><strong>Fixation</strong>. Fixations are when we focus on something for a period of time. Fixations range in duration from tens of milliseconds to several seconds.</p><p><strong>IPD</strong>. Interpupillary distance. The distance between the centres of your eyes/pupils (whilst looking straight ahead, far into the distance). This value is often part of your eyeglass prescription.</p><p><strong>MAE</strong>. Mean absolute error. Used to characterize the quality of a calibration (the higher the MAE, the worse the calibration). MAE is calculated as part of a validation. The acceptable range of MAE depends on the type of calibration performed: the more points used during the calibration, the higher the expectations for its quality (and thus a lower MAE). For a 1 point calibration, the largest MAE ‘accepted’ is 2°; for a 9 point calibration, a 1° MAE is the upper limit.</p><p><strong>Saccade</strong>. The rapid movement of both eyes to shift the centre of gaze to a new portion of the visual field.</p><p><strong>Scene</strong>. Within Unity, a scene contains the objects for your game or application. A single game or application can have multiple scenes; tying them together allows you to build a story or change contexts.</p><p><strong>Tracker</strong>. The collective term for the hardware and software used to track a single eye.</p><p><strong>Quick Start</strong>. A process to get the eye tracker set up quickly. You can trigger a Quick Start via the API.</p><p><strong>Validation</strong>. The process of having a user look at a series of targets post-calibration. By comparing the calculated gaze to known target position, the validity of the calibration can be assessed.</p><p><strong>VOR</strong>. Vestibulo–ocular reflex. Reflex triggered by the vestibular system (related to the inner ear and orientation sensing) to ensure that the visual signal to the eye is stabilized during movement. One of the calibration modes (‘VOR’) we use involves keeping your gaze fixed on a point and moving your head to align a ‘cursor’ to that fixed marker; this takes advantage of the VOR. See https://en.wikipedia.org/wiki/Vestibulo%E2%80%93ocular_reflex</p></section>
      <footer>
        <p>Copyright 2022 AdHawk Microsystems Inc</p>
        <p>STRICTLY CONFIDENTIAL</p>
      </footer>
    </div>
  </div>
  <div>
      </div>
</body>
</html>
