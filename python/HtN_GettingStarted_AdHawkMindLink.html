<!--Autogenerated using pandoc <md files> -o <outfile.html> --toc --template template.html5 --css style.css  --section-divs -->

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Getting started with AdHawk MindLink</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
  <div>
    <div id="ah-sidebar">
      <ul id="toc">
                <ul><li><a href="#getting-started-with-adhawk-mindlink">Getting started with AdHawk MindLink</a></li><li><a href="#what-is-eye-tracking">What is eye tracking?</a><ul><li><a href="#human-eye-movements">Human eye movements</a></li><li><a href="#how-does-eye-tracking-work">How does eye tracking work?</a></li><li><a href="#eye-tracking-applications">Eye tracking applications</a></li></ul></li><li><a href="#how-does-adhawk-eye-tracking-work">How does AdHawk eye tracking work?</a><ul><li><a href="#hardware-components">Hardware components</a></li><li><a href="#software-components">Software components</a><ul><li><a href="#adhawk-backend">AdHawk Backend</a></li></ul></li></ul></li><li><a href="#system-requirements">System requirements</a></li><li><a href="#installing-the-sdk-and-examples">Installing the SDK and examples</a></li><li><a href="#fitting-your-mindlink-glasses">Fitting your MindLink glasses</a><ul><li><a href="#changing-the-nosepiece">Changing the nosepiece</a><ul><li><a href="#a-proper-fit">A proper fit</a></li><li><a href="#selecting-a-nosepiece">Selecting a nosepiece</a></li><li><a href="#installing-a-nosepiece">Installing a nosepiece</a></li></ul></li><li><a href="#using-ear-hooks">Using ear hooks</a></li><li><a href="#prescription-lenses">Prescription lenses</a></li></ul></li><li><a href="#quick-start">Quick Start</a></li><li><a href="#auto-tune">Auto-tune</a></li><li><a href="#calibration">Calibration</a></li><li><a href="#device-calibration">Device calibration</a></li><li><a href="#testing-the-quality-of-your-setup-with-validation">Testing the quality of your setup with validation</a></li><li><a href="#creating-an-eye-tracking-app">Creating an eye tracking app</a><ul><li><a href="#adhawk-python-api">AdHawk Python API</a><ul><li><a href="#return-codes">Return codes</a></li><li><a href="#dependencies">Dependencies</a></li><li><a href="#example-code">Example code</a></li><li><a href="#data-streams">Data streams</a></li><li><a href="#asynchronous-vs-synchronous-operation">Asynchronous vs synchronous operation</a></li></ul></li></ul></li><li><a href="#what-are-the-important-steps-within-adhawk-eye-tracking">What are the important steps within AdHawk eye tracking?</a></li><li><a href="#troubleshooting">Troubleshooting</a><ul><li><a href="#mindlink-camera-preview-is-not-working-properly">MindLink camera preview is not working properly</a></li><li><a href="#one-or-both-eyes-arent-found-during-a-quick-start-or-auto-tune.">One or both eyes aren’t found during a Quick Start or Auto-tune.</a></li></ul></li><li><a href="#glossary">Glossary</a></li></ul>
              </ul>
    </div>
    <div id="ah-main">
<!--https://stackoverflow.com/questions/42773587/code-block-formatting-issues-with-bootstrap-and-pandoc-->
<section id="getting-started-with-adhawk-mindlink" class="level1"><h1>Getting started with AdHawk MindLink</h1></section><section id="what-is-eye-tracking" class="level1"><h1>What is eye tracking?</h1><p>Eye tracking is the process of observing eye behavior so you can learn about the point of gaze. Eye tracking can be used to:</p><ul><li>learn what someone is looking at</li><li>learn about aspects of someone’s mental state (e.g., fatigue, concussion, level of focus, etc.)</li></ul><section id="human-eye-movements" class="level2"><h2>Human eye movements</h2><p>Before diving into the AdHawk SDK and writing an application that incorporates eye tracking data, it helps to know about some of the more common eye movements.</p><section id="fixations" class="level4"><h4>Fixations</h4><p>One of the most common eye events occurs when the eyes aren’t moving. <em>Fixations</em> occur when we focus on something for a period of time. Fixations range in duration from tens of milliseconds to several seconds.</p></section><section id="saccades" class="level4"><h4>Saccades</h4><p><em>Saccades</em> occur when our eyes move between fixations. These movements are the fastest movements the human body can perform; they can reach a peak velocity of 500 degrees/second (although the amplitude of saccades are usually in the 4 to 20 degree range).</p><blockquote><p>Saccades are an event within the AdHawk API—you can register to receive notifications when they occur.</p></blockquote></section><section id="smooth-pursuit" class="level4"><h4>Smooth pursuit</h4><p><em>Smooth pursuit</em> is another form of eye movement but one which requires something to follow (unlike saccades which can happen without stimuli). Consider a bird flying across the sky: our brain enters a feedback loop where we can smoothly track the bird as it moves. Smooth pursuit velocities range from 10 to 30 degrees/second (compared with the much faster saccades).</p></section><section id="blinks" class="level4"><h4>Blinks</h4><p>Blinking closes the eyelids to moisten the eye. We often think of blinking as not involving eyeball movement, but while fixating on something straight ahead, the eyeball actually rotates downward slightly during a blink.</p><blockquote><p><a href="https://adhawkmicrosystems.github.io/api/api.html#blink-events">Blinks are an event within the AdHawk API</a>; you can register to receive notifications when they occur.</p></blockquote></section><section id="vestibulo-ocular-reflex-vor" class="level4"><h4>Vestibulo-ocular reflex (VOR)</h4><p>The vestibulo-ocular reflex acts to stablize your gaze (on a visual target) while your head is moving. This helps stabilize the image on your retina as the environment may be changing. To see VOR in action, fixate on a word in this sentence and move your head from right to left; notice how your gaze remains locked to the word while your head moves?</p><p>One of the calibration methods supported by AdHawk relies on VOR (‘fixed gaze’).</p></section><section id="vergence" class="level4"><h4>Vergence</h4><p><em>Vergence</em> involves eyes moving in opposite directions. This is done to converge or diverge, usually to focus on items that are close up or far away, respectively.</p><blockquote><p>Vergence information is available as part of the <a href="#gaze-stream">gaze data stream</a>.</p></blockquote></section></section><section id="how-does-eye-tracking-work" class="level2"><h2>How does eye tracking work?</h2><p>You can generally tell where someone is looking by looking at their eyes. Eye tracking is nothing more than a really accurate, really fast way of the same thing.</p><p>Some eye tracking systems use a combination of small cameras pointed at your eyes and image analysis to figure out where you’re looking.</p><p>AdHawk eye tracking doesn’t use cameras and image processing (both require a fair bit of power), instead using incredibly small (MEMS-scale) mirrors to shine a lower power IR light across your eyes. Reflections from your eyes are captured by photodetectors and feed into a mathematical model. This model allows us to determine where you’re looking very accurately (&lt; 1 degree MAE) and at high rates (up to 500 Hz).</p><figure><img src="./assets/htn/mems.png" width="400" alt="" /><figcaption>MEMS mirror</figcaption></figure></section><section id="eye-tracking-applications" class="level2"><h2>Eye tracking applications</h2><p>Eye tracking can be used in several different ways:</p><ul><li><strong>Interaction</strong>. In the same way as a mouse or your finger can move across a screen, so to can your gaze act as a pointer. Consider actions like blinks to be like mouse clicks or taps. This sort of direct interaction can be as rich as other modalities, and can allow us to communicate our interest and intent through eye behavior.</li><li><strong>Observation</strong>. Eye behavior is becoming increasingly valuable in unlocking mental and cognitive states. Eye tracking can be used to observe and collect information about these states in an unobtrusive way. Data collected can be used to help identify, diagnose, or assess recovery for certain medical conditions (e.g., concussion, dementia, etc.).</li><li><strong>Research</strong>. Although a specialized form of observation, eye tracking has been used in market research (to learn more about people’s behavior in retail environments, for example) and lab settings (e.g., psychology, neuroscience, usability, etc.) for a long time.</li></ul><p>We’ve posted a few interesting examples of eye tracking on our <a href="https://sites.google.com/adhawkmicrosystems.com/hack-the-north-help-center/home">Hack the North website</a>.</p></section></section><section id="how-does-adhawk-eye-tracking-work" class="level1"><h1>How does AdHawk eye tracking work?</h1><p>Traditional eye tracking systems point a camera at your eye and use the image of your pupil to determine where you’re looking.</p><p>The AdHawk eye tracking system does not use a camera to calculate your gaze. Instead, a path of low power IR light is drawn on your eyeball and photodetectors capture the reflections. Those glints tell enough about the position and orientation of your eyes to figure out where you’re looking.</p><p>Another key component of the MindLink glasses is the front-facing camera. This captures the scene and helps with correlating your gaze with something you may be looking at (this is especially important during calibration, but also great for seeing what someone is looking at or hoping to interact with).</p><p>With this approach we can calculate your gaze (where you’re looking), the vergence angle (angle between your individual eye gaze vectors), and pupil size and position.</p><p><a href="https://www.youtube.com/watch?v=E_i-wjgFLhc"><img src="https://img.youtube.com/vi/E_i-wjgFLhc/0.jpg" alt="MindLink introduction video" /></a></p><section id="hardware-components" class="level2"><h2>Hardware components</h2><p>The hardware components within the AdHawk eye tracking system are:</p><ol type="1"><li>Scanner. A MEMS device that directs low power IR light across your eyes.</li><li>Photodetectors. Sensors that pick up on the IR light reflections.</li><li>Front-facing camera. Also known as a world or scene camera; captures the view of the MindLink wearer so that gaze can be overlaid, markers can be detected, etc.</li></ol><p>Some important features and specifications of the AdHawk MindLink system include:</p><table><thead><tr class="header"><th>Feature</th><th>Specification</th></tr></thead><tbody><tr class="odd"><td>Calibrated range</td><td>40 × 25 degrees (width × height)</td></tr><tr class="even"><td>Gaze Mean Absolute Error</td><td>&lt;1.0 degrees</td></tr><tr class="odd"><td>Gaze data rate</td><td>Up to 500 outputs per second</td></tr><tr class="even"><td>Calibration points</td><td>user configurable from 1–9</td></tr><tr class="odd"><td>Latency</td><td>3 ms</td></tr><tr class="even"><td>Weight</td><td>27 g</td></tr></tbody></table><p>Eye tracking features:</p><ul><li>binocular gaze tracking and output</li><li>vergence output</li><li>pupil size output</li><li>robust tracking under sunlight</li><li>single click eye tracking</li><li>slip tolerant eye tracking</li><li>export eye tracking video</li><li>export eye tracking data</li><li>gaze validation function</li></ul><p>Front-facing camera:</p><ul><li>82 degree FOV</li><li>16:9 aspect ratio</li><li>supports 720p, 1080p @ 30 FPS</li></ul><p>Prescription lenses:</p><ul><li>a limited number of prescription lenses (standard diopter, non-astigmatism) are available from AdHawk folks at their sponsor booth</li></ul></section><section id="software-components" class="level2"><h2>Software components</h2><p>Depending upon which platform you’re building with, your software stack will look a bit different.</p><p>With an AdHawk MindLink, your minimal stack will include:</p><ul><li>your application, which relies in part on…</li><li>the AdHawk python SDK, which communicates with…</li><li>the AdHawk Backend</li></ul><figure><img src="./assets/htn/python_architecture.png" width="700" alt="" /><figcaption>Python architecture</figcaption></figure><p>You may find you use other libraries or code to help with things like a GUI or visualization. Some of the sample applications we’ve provided rely on Qt (for the interface), or other third party libraries (e.g., numpy, OpenCV). Those can be installed using the included <code>requirements.txt</code> file for each example.</p><section id="adhawk-backend" class="level3"><h3>AdHawk Backend</h3><p>The AdHawk Backend program acts as a go-between for the MindLink glasses and applications who want to use eye tracking. In order to receive eye tracking data streams in your application, AdHawk Backend needs to be running.</p><p>Run AdHawk Backend from your computer. After launching, the AdHawk Backend icon <img src="../assets/backend/BackendIcon.png" width="16" /> will appear in the system tray. Keep the AdHawk Backend service running in the background while eye tracking.</p><p>The AdHawk Backend will continue to run until you close it. You can launch, run, and close down your application repeatedly with the same instance of AdHawk Backend running. The service can be shutdown by right clicking on the system tray icon, and selecting <em>Quit AdHawk Backend</em>.</p></section></section></section><section id="system-requirements" class="level1"><h1>System requirements</h1><p>To create an eye tracking application using the AdHawk MindLink glasses, you will need a computer that:</p><ul><li>has an Intel Core i5 or equivalent CPU</li><li>has 8 GB RAM or more</li><li>has 1 GB disk storage free</li><li>has 1 free USB port (adapters and cables provided as part of the kit)</li><li>runs Windows 10, Windows 11, or Linux</li></ul></section><section id="installing-the-sdk-and-examples" class="level1"><h1>Installing the SDK and examples</h1><p>To install the Python SDK:</p><pre><code>pip install adhawk</code></pre><blockquote><p><strong>NOTE</strong>: Python 3.9.5 is the version you’ll want to use.</p></blockquote><p>This will install the contents of the SDK to your python <code>site-packages</code> directory. From there, you can <code>import adhawkapi</code>, <code>import adhawkapi.frontend</code>, etc. into your python code.</p><p>To install the example programs, visit the <a href="https://github.com/adhawk-microsystems/python-sdk-examples">AdHawk github page</a>. Clone the repo so you’ve got the code locally.</p><ul><li>we recommend you work within a <a href="https://docs.python.org/3/tutorial/venv.html">Python virtual environment</a></li><li>don’t forget to install the dependencies of a given sample application (see the requirements.txt in each example’s directory); this is also described by the <a href="https://github.com/adhawk-microsystems/python-sdk-examples/blob/master/README.md">README.md</a> for the github repo</li></ul></section><section id="fitting-your-mindlink-glasses" class="level1"><h1>Fitting your MindLink glasses</h1><p>Eye tracking systems are largely comprised of physical sensors positioned near the eyes themselves. To ensure great eye tracking, you need to optimize the position of those sensors.</p><p>You can adjust the fitting of MindLink glasses by: * changing the nosepiece * using ear hooks</p><blockquote><p>If you can’t seem to get a good fit with any of the nosepieces or suggestions, visit the AdHawk sponsor booth.</p></blockquote><section id="changing-the-nosepiece" class="level2"><h2>Changing the nosepiece</h2><p>The MindLink kit ships with a set of four different nosepieces. You can tell the nosepieces from each other by their size, but also the small colour marking. You should use the correct nose piece for each user to improve eye tracking.</p><figure><img src="./assets/htn/mindlink_nosepiece_group_side.png" width="600" alt="" /><figcaption>Side view of four MindLink nosepieces</figcaption></figure><figure><img src="./assets/htn/mindlink_nosepiece_group_top.png" width="600" alt="" /><figcaption>Back view of four MindLink nosepieces</figcaption></figure><section id="a-proper-fit" class="level3"><h3>A proper fit</h3><p>When fitting the glasses, your goal is to have the pupil located midway between the top and bottom of the frame.</p><figure><img src="./assets/htn/mindlink_nosepiece_front.png" width="600" alt="" /><figcaption>Correct Fit Example: Front</figcaption></figure></section><section id="selecting-a-nosepiece" class="level3"><h3>Selecting a nosepiece</h3><p>Each nosepiece can: * change the height at which the glasses sit on your face * change the distance the glasses sit from your eye (the ‘relief’) * change both the height and relief</p><p>As you increase the nosepiece number from 1 (pink) up to 4 (blue), the glasses will get higher and farther away from your face. The table below describes the change in glasses fit between the four nosepieces.</p><table><thead><tr class="header"><th>Nosepiece</th><th>Purpose</th></tr></thead><tbody><tr class="odd"><td>1 (pink)</td><td>Reduce height</td></tr><tr class="even"><td>2 (yellow)</td><td>Reduce relief (distance from face)</td></tr><tr class="odd"><td>3 (black)</td><td>This is the baseline nosepiece</td></tr><tr class="even"><td>4 (blue)</td><td>Increase height</td></tr></tbody></table><p>The nosepieces are designed to accommodate different face and nose shapes, to maintain a consistent glasses position relative to the eyeball. They follow the trend shown below:</p><figure><img src="./assets/htn/nosepieces_trend.png" width="600" alt="" /><figcaption>Face shapes and recommended nosepieces</figcaption></figure></section><section id="installing-a-nosepiece" class="level3"><h3>Installing a nosepiece</h3><p>You can swap the nosepiece by gently pinching the nose pads towards each other and sliding the bridge of the nosepiece into or out of the channel on the glasses. Use your index finger of the opposite hand to guide the nosepiece into place. The nosepiece will snap in place once inserted fully.</p><figure><img src="./assets/htn/insert_nosepiece-sm.png" width="600" alt="" /><figcaption>Installing a nosepiece by pinching it</figcaption></figure></section></section><section id="using-ear-hooks" class="level2"><h2>Using ear hooks</h2><p>Your MindLink kit includes two ear hooks. Install the ear hooks on the arms of the glasses to reduce slipping. The ear hooks should work to maintain an optimal fitting, not work against your nosepiece selection.</p><p>To install the ear hooks: * Slide the ear hook opening over the end of the glasses’ arm, starting with the pointed upper edge of the arm. * Rotate the ear hook downwards to fully enclose the end of the arm with the ear hook. * Slide and wiggle the ear hook until you’ve moved it to the desired location on the arm.</p><figure><img src="./assets/htn/installing_earhooks-sm.png" width="600" alt="" /><figcaption>MindLink glasses with ear hooks installed</figcaption></figure></section><section id="prescription-lenses" class="level2"><h2>Prescription lenses</h2><p>AdHawk MindLink glasses can have their non-prescription lenses swapped out for prescription lenses. A limited number of prescription lenses (standard diopter, non-astigmatism) are available from AdHawk folks at their sponsor booth if you need them.</p></section></section><section id="quick-start" class="level1"><h1>Quick Start</h1><p>The <em>Quick Start</em> activity is intended to get your eye tracking up and running quickly. It works best with a saved calibration.</p></section><section id="auto-tune" class="level1"><h1>Auto-tune</h1><p>The <em>Auto-tune</em> activity will scan your eye, then tweak the size and position of the scan box used during eye tracking. Auto-tune is part of Quick Start, but can also be run independently.</p></section><section id="calibration" class="level1"><h1>Calibration</h1><p><em>Calibration</em> reconciles the gaze with the real world. To calibrate, a marker—something to fixate on—is shown on a monitor. Using the front-facing camera, we can determine where is it. Then we capture the gaze when someone is looking at the marker. We create a model that allows us to map a measured gaze with a position in the real world.</p></section><section id="device-calibration" class="level1"><h1>Device calibration</h1><p>Infrequently, the eye tracker needs to be calibrated against a known environment or set of inputs. This is different than regular calibration (see above) and involves a calibration <em>fixture</em>. If you need to perform a device calibration (e.g., you get a return code of 17 when calling into the API), visit AdHawk’s sponsor booth.</p></section><section id="testing-the-quality-of-your-setup-with-validation" class="level1"><h1>Testing the quality of your setup with validation</h1><p>If calibration allows us to create a model to map gaze to real world objects, validation is a way of testing that model to see how accurate it is. The process of validation is very similar to calibration in that it involves looking at markers in a known location in the real world.</p><p>Validation does not need to be done prior to performing eye tracking, but it is a good way to see if you’re set up properly. Aim for a mean absolute error (MAE) of 1 degree or less for the best experience with eye tracking.</p></section><section id="creating-an-eye-tracking-app" class="level1"><h1>Creating an eye tracking app</h1><p>Your application can use the AdHawk MindLink glasses by using the Python software development kit (SDK). The SDK includes APIs to trigger a calibration, register for gaze streams, and so on.</p><section id="adhawk-python-api" class="level2"><h2>AdHawk Python API</h2><section id="return-codes" class="level3"><h3>Return codes</h3><p>When using the Python SDK, return codes may be sent back to your calling code. These provide status information about the call you’ve made or the system in general.</p><p>If a call is successful, the ack code will be SUCCESS (0). Non-zero ack codes indicate something went wrong or a situation where additional setup work is required.</p><pre><code>SUCCESS = 0
FAILURE = 1
INVALID_ARGUMENT = 2
TRACKER_NOT_READY = 3
EYES_NOT_FOUND = 4
RIGHT_EYE_NOT_FOUND = 5
LEFT_EYE_NOT_FOUND = 6
NOT_CALIBRATED = 7
NOT_SUPPORTED = 8
SESSION_ALREADY_RUNNING = 9
NO_CURRENT_SESSION = 10
REQUEST_TIMEOUT = 11
UNEXPECTED_RESPONSE = 12
HARDWARE_FAULT = 13
CAMERA_FAULT = 14
BUSY = 15
COMMUNICATION_ERROR = 16
DEVICE_CALIBRATION_REQUIRED = 17</code></pre><p>You can find a full list of ack codes in the <a href="todo">API documentation</a>.</p><p>A <strong>DEVICE_CALIBRATION_REQUIRED</strong> (17) ack code indicates that the AdHawk MindLink device itself needs to be calibrated. This doesn’t happen often and is different than a calibration that occurs when setting up eye tracking for a specific user. If you receive this ack code, bring your glasses to the AdHawk sponsor booth so they can be calibrated.</p></section><section id="dependencies" class="level3"><h3>Dependencies</h3><p>You may need to install dependencies as you use the example programs or build out your application. Some common libraries include:</p><ul><li>Qt (via <code>PySide2</code>; we use version 5.15.12)</li><li><code>pyqtgraph</code>; we use version 0.12.1</li><li><code>numpy</code>; we use version 1.20.3</li><li>OpenCV (via <code>opencv-contrib-python-headless</code>; we use version 4.5.2.52)</li></ul></section><section id="example-code" class="level3"><h3>Example code</h3><p>There are three sample applications to show you how to use key parts of the AdHawk MindLink python SDK:</p><section id="simple-data-streaming-stream_register_example.py" class="level4"><h4>Simple data streaming (stream_register_example.py)</h4><p>This example demonstrates:</p><ul><li>how to subscribe to gaze and event streams</li><li>how to and handle data from gaze and event streams; data is dumped to the console</li></ul><p>The example is written in Python and only depends on the AdHawk Python SDK. Code can be found on <a href="https://github.com/adhawk-microsystems/python-sdk-examples">AdHawk’s github</a>.</p></section><section id="gaze-in-image-camera_gaze_example.py" class="level4"><h4>Gaze in image (camera_gaze_example.py)</h4><p>This example demonstrates:</p><ul><li>how to handle video from the MindLink front-facing camera</li><li>how to overlay a gaze marker onto the video stream properly</li></ul><p>The example is written in Python and depends on the AdHawk Python SDK and PySide2 (for Qt/the GUI). Code can be found on <a href="https://github.com/adhawk-microsystems/python-sdk-examples">AdHawk’s github</a>.</p><section id="parallax" class="level5"><h5>Parallax</h5><p>Parallax correction aims to bring the projected gaze and the camera image into the same focal plane. This ensures that the gaze position is accurate when mapped onto that image. If your calibration seems good but your gaze marker when mapped into the camera image seems off, you may need to adjust the gaze depth.</p><p>There are two settings that relate to parallax; both are adjusted using <code>set_camera_user_settings()</code>.</p><p><code>GAZE_DEPTH</code> (where value is the gaze depth in metres)</p><pre><code>api.set_camera_user_settings(adhawkapi.CameraUserSettings.GAZE_DEPTH, 0.5)</code></pre><p><code>PARALLAX_CORRECTION</code> (where 1 is enable—and the default—and 0 is disable)</p><pre><code>api.set_camera_user_settings(adhawkapi.CameraUserSettings.PARALLAX_CORRECTION, 1)</code></pre><blockquote><p>See the <a href="https://adhawkmicrosystems.github.io/api/api.html#camera-user-settings">API documentation</a> for more information about these calls.</p></blockquote></section></section><section id="screen-tracking-screen_tracking_example.py" class="level4"><h4>Screen tracking (screen_tracking_example.py)</h4><p>This example demonstrates:</p><ul><li>how to track gaze on a monitor</li><li>how to generate ArUco markers and place them on screen (these are used by the MindLink camera for spatial positioning)</li></ul><p>n.b. You may need to change the camera index in the call to <code>start_camera_capture</code>. The index you need to use will be system dependent based on which other webcams, etc. you have attached, but is often 0 or 1.</p><p>The example is written in Python and depends on OpenCV (for marker generation and recognition), and PySide2 (for Qt/the GUI).Code can be found on <a href="https://github.com/adhawk-microsystems/python-sdk-examples">AdHawk’s github</a></p></section></section><section id="data-streams" class="level3"><h3>Data streams</h3><p>There are several streams of eye tracking data available. Some streams are always broadcast, such as the signal indicating if the eye tracker is ready. Other streams need to be subscribed to individually (like the gaze stream).</p><section id="gaze-stream" class="level4"><h4>Gaze stream</h4><p>The best way to learn how to use the gaze stream data is by looking at <code>stream_register_example.py</code> (for a simple example of how to register and receive the gaze data stream).</p><pre><code>def handle_gaze_data(*data):
    timestamp, xpos, ypos, zpos, vergence = data

self._api.register_stream_handler(
  adhawkapi.PacketType.GAZE,
  self._handle_gaze_data)

self._api.set_stream_control(
  adhawkapi.PacketType.GAZE,
  1,
  callback=(lambda *_args: None))</code></pre><p>Gaze data is comprised of: - timestamp since the system started, in seconds (float32) - x position, in meters (float32) - y position, in meters (float32) - z position, in meters (float32)</p><p>Note that x, y, and z positions are the estimated coordinates of the user’s gaze point relative to the <em>midpoint of the scanners</em>. More information about the AdHawk coordinate system and its relationship to the AdHawk MindLink glasses can be found in the <a href="TODO">API documentation</a>.</p></section><section id="event-stream" class="level4"><h4>Event stream</h4><p>Several higher level eye events are recognized and made available as part of the event stream.</p><ul><li><strong>Blink</strong>. This event is triggered as soon as either of the two blinking eyes opens. The blink event indicates the time window where both left and right eye blink events overlap in time.</li></ul><blockquote><p>Blink events include a timestamp and duration (ms). See the <a href="todo">API documentation</a> for more details. Blink events are also used in <code>stream_register_example.py</code>, one of the example programs.</p></blockquote><ul><li><strong>Saccade</strong>. This event is triggered as soon as either of the saccades (left or right in a combined-eye saccade) ends. The saccade event indicates the time window where both left and right saccade events overlap in time. This event is triggered as soon as the saccade in any of the eyes ends.</li></ul><blockquote><p>Saccade events include a timestamp (of the end of the saccade), duration (ms), and an amplitude (degrees). For more information, see the <a href="todo">API documentation</a>.</p></blockquote><p>These events can be used for interaction or captured as a way to record eye behavior.</p></section><section id="gaze-in-screen-stream" class="level4"><h4>Gaze in screen stream</h4><p>This stream provides normalized (x, y) coordinates of the gaze inside the screen. The screen is defined by a set of ArUco markers, often in the corners of the screen. x and y are float values (0–1) where (0, 0) is the top-left corner and (1, 1) is the bottom-right corner of the screen.</p><pre><code>def handler(*data):
    timestamp, xpos, ypos = data

api.register_stream_handler(adhawkapi.PacketType.GAZE_IN_SCREEN, handler)</code></pre><blockquote><p>For more information, see the <a href="">API documentation</a> or the <code>screen_tracking_example.py</code> example source code.</p></blockquote></section><section id="imu-stream" class="level4"><h4>IMU stream</h4><p>The MindLink glasses contain an inertial measurement unit (IMU). There are two data streams associated with the IMU:</p><ul><li>IMU data stream</li><li>IMU rotation data stream</li></ul><p>For more information about the IMU data streams, see the <a href="./api.html#imu-data-stream-0x17">API documentation</a>.</p></section></section><section id="asynchronous-vs-synchronous-operation" class="level3"><h3>Asynchronous vs synchronous operation</h3><p>You can chose to make Python API calls as either blocking or non-blocking.</p><p>To perform a non-blocking (or asynchronous) operation, provide a callback function as a parameter to be executed when the operation is complete. This mode of operation is useful for GUI applications.</p><p>If a callback function is not provided, the call blocks until a response is received. On success, the response is returned. On failure, an exception is raised.</p><p>For examples of both types of calling, see the <a href="api.html#synchronous-vs-aysnchronous-operation">API documentation</a>.</p></section></section></section><section id="what-are-the-important-steps-within-adhawk-eye-tracking" class="level1"><h1>What are the important steps within AdHawk eye tracking?</h1><p>There are a few steps required to get eye tracking working well within your application:</p><ol type="1"><li>Look at the section on fitting your MindLink glasses. You may need to tweak things as you go, but at least find a nosepiece that seems reasonable.</li><li>Connect your MindLink glasses to your computer.</li><li>Start AdHawk Backend. This application runs in the background but will be accessible via the System Tray (on Windows).</li><li>Establish a connection with AdHawk Backend (in code; see the example programs for the best way to do this). This step will allow you to communicate with the eye tracker itself.</li><li>Run a Quick Start and/or calibration to create a model that can be used for eye tracking.</li><li>Start developing your application!</li></ol></section><section id="troubleshooting" class="level1"><h1>Troubleshooting</h1><section id="mindlink-camera-preview-is-not-working-properly" class="level2"><h2>MindLink camera preview is not working properly</h2><p>In order to start capturing frames from the front-facing MindLink camera, you need to correctly indicate its camera device index. If there are multiple cameras connected (e.g., a webcam in addition to the MindLink glasses), the device index might need to be discovered by trial and error.</p><pre><code>api.start_camera_capture(camera_device_index, resolution_index, undistort_image, callback=handle_camera_start_response)</code></pre><p>If the MindLink camera images are not displayed using a specific device index, then another device index should be tried.</p></section><section id="one-or-both-eyes-arent-found-during-a-quick-start-or-auto-tune." class="level2"><h2>One or both eyes aren’t found during a Quick Start or Auto-tune.</h2><p>More often than not, when this occurs, your glasses aren’t fitting properly. Check that you’re using the correct nosepiece <strong>for you</strong>, that your eyes are centered within the lenses (when viewed from the front), and that the glasses aren’t slipping down your nose.</p><p>Occasionally, the windows covering the scanner will be dirty. Gently clean the window surface with the provided cloth.</p><figure><img src="./assets/htn/clean_sensor_windows.png" width="600" alt="" /><figcaption>Sensor windows</figcaption></figure></section></section><section id="glossary" class="level1"><h1>Glossary</h1><p><strong>AdHawk MindLink</strong>. AdHawk MindLink glasses include a full eye tracking system within the frames.</p><p><strong>Anti-saccade</strong>. A saccade <em>away</em> from a stimulus.</p><p><strong>ArUco markers</strong>. Specialized (graphical) markers designed to facilitate computer vision. They are blocky black and white grids with a black border.</p><p><strong>Auto-tune</strong>. A process to optimize where the eye tracking system will scan on someone’s eyes.</p><p><strong>Calibration</strong>. The process by which the relationship between a user’s eye tracking setup—and thus their gaze—and the world is calculated, allowing for the ‘real world’ gaze target to be determined.</p><p><strong>Device calibration</strong>. Infrequently, the eye tracker needs to be calibrated against a known environment or set of inputs. This is different than regular calibration (see above) and involves a calibration <em>fixture</em>. If you need to perform a device calibration (e.g., you get a return code of 17 when calling into the API), visit AdHawk’s sponsor booth.</p><p><strong>Eye tracking</strong>. The process of inferring the gaze point (and, in general, eye behavior) by using technology to observe someone’s eyes.</p><p><strong>Fixation</strong>. Fixations are when we focus on something for a period of time. Fixations range in duration from tens of milliseconds to several seconds.</p><p><strong>IMU</strong>. Inertial measurement unit. The IMU is a sensor used to measure orientation and acceleration.</p><p><strong>IPD</strong>. Interpupillary distance. The distance between the centres of your eyes/pupils (whilst looking straight ahead, far into the distance). This value is often part of your eyeglass prescription.</p><p><strong>MAE</strong>. Mean absolute error. Used to characterize the quality of a calibration (the higher the MAE, the worse the calibration). MAE is calculated as part of a validation. The acceptable range of MAE depends on the type of calibration performed: the more points used during the calibration, the higher the expectations for its quality (and thus a lower MAE). For a 1 point calibration, the largest MAE ‘accepted’ is 2°; for a 9 point calibration, a 1° MAE is the upper limit.</p><p><strong>Saccade</strong>. The rapid movement of both eyes to shift the centre of gaze to a new portion of the visual field.</p><p><strong>Tracker</strong>. The collective term for the hardware and software used to track a single eye.</p><p><strong>Quick Start</strong>. A process to get the eye tracker set up quickly. You can trigger a Quick Start via the API.</p><p><strong>Validation</strong>. The process of having a user look at a series of targets post-calibration. By comparing the calculated gaze to known target position, the validity of the calibration can be assessed.</p><p><strong>VOR</strong>. Vestibulo–ocular reflex. Reflex triggered by the vestibular system (related to the inner ear and orientation sensing) to ensure that the visual signal to the eye is stabilized during movement. One of the calibration modes (‘VOR’) we use involves keeping your gaze fixed on a point and moving your head to align a ‘cursor’ to that fixed marker; this takes advantage of the VOR. See https://en.wikipedia.org/wiki/Vestibulo%E2%80%93ocular_reflex</p></section>
      <footer>
        <p>Copyright 2022 AdHawk Microsystems Inc</p>
        <p>STRICTLY CONFIDENTIAL</p>
      </footer>
    </div>
  </div>
  <div>
      </div>
</body>
</html>
